# Hand Gesture Recognition for Arabic Sign Language to Text

## Team Members  
- **Seif Sherif Assad Ali**  
- **Sama Ahmed ElSayed**  
- **Yusuf Sobhy Sadek Elmeligy**  
- **George Nemr Mellek Poqtor**  
- **Omar Elsayed Elsayed Mousa**  

## Project Overview 
This project focuses on developing a **Hand Gesture Recognition System** that translates **Arabic Sign Language (ArSL)** into **text prompts** using deep learning and computer vision. The goal is to create an accessible tool for individuals who rely on sign language for communication, enhancing inclusivity and bridging the gap between the deaf and hearing communities.  

## Motivation  
Millions of people use Arabic Sign Language (ArSL) as their primary means of communication, yet there is a significant gap in accessibility tools tailored to the Arabic-speaking deaf community. This project aims to **leverage AI and machine learning** to provide a practical solution that can **recognize hand gestures and convert them into text in real time**.  

## Project Scope & Features  
- **Dataset:** RGB images of Arabic Sign Language letters.  
- **Preprocessing:** Image cleaning, resizing, augmentation, and normalization.  
- **Model:** Convolutional Neural Networks (CNNs) for hand gesture classification.  
- **Output:** Real-time conversion of gestures into Arabic text.  
- **Future Expansion:** Potential integration with speech synthesis for text-to-speech conversion.  

## Project Goals  
✔ Develop a robust **gesture recognition model** trained on Arabic sign language datasets.  
✔ Ensure **high accuracy** in gesture-to-text conversion.  
✔ Implement a **user-friendly interface** for real-time interaction.  
✔ Promote **accessibility** and **inclusivity** for the deaf and hard-of-hearing community.  

## Deep Learning Models
https://drive.google.com/drive/folders/14_aZ9G3FArZuQPstq-lvlq-59tF3Immg?usp=sharing
